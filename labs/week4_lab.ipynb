{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cec22089-64ac-4020-b788-88c58c29cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdptoolbox import example\n",
    "import mdptoolbox\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8b7585-f394-4f20-9be7-e508c70416dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R = example.forest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ac5124-07db-4cb4-bb85-6d4fb1980b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(P), type(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28180ff3-9a47-45a5-9d5d-7f24a0b1ad54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 3) (3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(P.shape, R.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec172cf6-a24e-4aa5-bca2-7a51f4b6f670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mmdptoolbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValueIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdiscount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minitial_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A discounted MDP solved using the value iteration algorithm.\n",
       "\n",
       "Description\n",
       "-----------\n",
       "ValueIteration applies the value iteration algorithm to solve a\n",
       "discounted MDP. The algorithm consists of solving Bellman's equation\n",
       "iteratively.\n",
       "Iteration is stopped when an epsilon-optimal policy is found or after a\n",
       "specified number (``max_iter``) of iterations.\n",
       "This function uses verbose and silent modes. In verbose mode, the function\n",
       "displays the variation of ``V`` (the value function) for each iteration and\n",
       "the condition which stopped the iteration: epsilon-policy found or maximum\n",
       "number of iterations reached.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "transitions : array\n",
       "    Transition probability matrices. See the documentation for the ``MDP``\n",
       "    class for details.\n",
       "reward : array\n",
       "    Reward matrices or vectors. See the documentation for the ``MDP`` class\n",
       "    for details.\n",
       "discount : float\n",
       "    Discount factor. See the documentation for the ``MDP`` class for\n",
       "    details.\n",
       "epsilon : float, optional\n",
       "    Stopping criterion. See the documentation for the ``MDP`` class for\n",
       "    details.  Default: 0.01.\n",
       "max_iter : int, optional\n",
       "    Maximum number of iterations. If the value given is greater than a\n",
       "    computed bound, a warning informs that the computed bound will be used\n",
       "    instead. By default, if ``discount`` is not equal to 1, a bound for\n",
       "    ``max_iter`` is computed, otherwise ``max_iter`` = 1000. See the\n",
       "    documentation for the ``MDP`` class for further details.\n",
       "initial_value : array, optional\n",
       "    The starting value function. Default: a vector of zeros.\n",
       "\n",
       "Data Attributes\n",
       "---------------\n",
       "V : tuple\n",
       "    The optimal value function.\n",
       "policy : tuple\n",
       "    The optimal policy function. Each element is an integer corresponding\n",
       "    to an action which maximises the value function in that state.\n",
       "iter : int\n",
       "    The number of iterations taken to complete the computation.\n",
       "time : float\n",
       "    The amount of CPU time used to run the algorithm.\n",
       "\n",
       "Methods\n",
       "-------\n",
       "run()\n",
       "    Do the algorithm iteration.\n",
       "setSilent()\n",
       "    Sets the instance to silent mode.\n",
       "setVerbose()\n",
       "    Sets the instance to verbose mode.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In verbose mode, at each iteration, displays the variation of V\n",
       "and the condition which stopped iterations: epsilon-optimum policy found\n",
       "or maximum number of iterations reached.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import mdptoolbox, mdptoolbox.example\n",
       ">>> P, R = mdptoolbox.example.forest()\n",
       ">>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.96)\n",
       ">>> vi.verbose\n",
       "False\n",
       ">>> vi.run()\n",
       ">>> expected = (5.93215488, 9.38815488, 13.38815488)\n",
       ">>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
       "True\n",
       ">>> vi.policy\n",
       "(0, 0, 0)\n",
       ">>> vi.iter\n",
       "4\n",
       "\n",
       ">>> import mdptoolbox\n",
       ">>> import numpy as np\n",
       ">>> P = np.array([[[0.5, 0.5],[0.8, 0.2]],[[0, 1],[0.1, 0.9]]])\n",
       ">>> R = np.array([[5, 10], [-1, 2]])\n",
       ">>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
       ">>> vi.run()\n",
       ">>> expected = (40.048625392716815, 33.65371175967546)\n",
       ">>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
       "True\n",
       ">>> vi.policy\n",
       "(1, 0)\n",
       ">>> vi.iter\n",
       "26\n",
       "\n",
       ">>> import mdptoolbox\n",
       ">>> import numpy as np\n",
       ">>> from scipy.sparse import csr_matrix as sparse\n",
       ">>> P = [None] * 2\n",
       ">>> P[0] = sparse([[0.5, 0.5],[0.8, 0.2]])\n",
       ">>> P[1] = sparse([[0, 1],[0.1, 0.9]])\n",
       ">>> R = np.array([[5, 10], [-1, 2]])\n",
       ">>> vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
       ">>> vi.run()\n",
       ">>> expected = (40.048625392716815, 33.65371175967546)\n",
       ">>> all(expected[k] - vi.V[k] < 1e-12 for k in range(len(expected)))\n",
       "True\n",
       ">>> vi.policy\n",
       "(1, 0)\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/torch2/lib/python3.9/site-packages/mdptoolbox/mdp.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     ValueIterationGS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mdptoolbox.mdp.ValueIteration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c79aa448-0aad-4196-a751-3cb2af65aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
    "vi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74b0056f-02af-40be-863e-d6d649abaf4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vi.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6b03a40-1ec8-4c2a-9d46-42166d96718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各状态价值 V = [ 4.69988075  1.29985425 -0.889095    0.        ]\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "P = np.array([\n",
    "    [0.5, 0.5, 0.0, 0.0],\n",
    "    [0.3, 0.3, 0.4, 0.0],\n",
    "    [0.0, 0.3, 0.3, 0.4],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "r = np.array([2, 0, -1, 0])  # 每个状态的即时奖励\n",
    "\n",
    "# 解MRP: V = (I - gamma*P)^(-1) * r\n",
    "I = np.eye(4)\n",
    "V = np.linalg.inv(I - gamma * P).dot(r)\n",
    "\n",
    "print(\"各状态价值 V =\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d436851e-6509-4907-a9e8-7b862e56b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P0 shape: (4, 4)\n",
      "P1 shape: (4, 4)\n",
      "P shape: (2, 4, 4)\n",
      "R shape: (2, 4)\n",
      "R_all_actions shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "# Action 0: Nap\n",
    "P0 = np.array([\n",
    "    [0.9, 0.1, 0.0, 0.0],\n",
    "    [0.1, 0.8, 0.1, 0.0],\n",
    "    [0.0, 0.2, 0.7, 0.1],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Action 1: Roam\n",
    "P1 = np.array([\n",
    "    [0.5, 0.5, 0.0, 0.0],\n",
    "    [0.3, 0.3, 0.4, 0.0],\n",
    "    [0.0, 0.3, 0.3, 0.4],\n",
    "    [0.0, 0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# 把这两个动作的转移概率打包成三维数组：P[a][s][s']\n",
    "# pymdptoolbox 里，P 是一个长度为 n_actions 的列表，每个元素是 (n_states x n_states) 的矩阵\n",
    "P = [P0, P1]\n",
    "\n",
    "# 奖励：假设仅依赖状态\n",
    "R = np.array([\n",
    "        [2, 0, -1, 0], # action 0\n",
    "        [1, 0, -2, 0]  # action 1\n",
    "])\n",
    "\n",
    "\n",
    "R_all_actions = R.transpose(1, 0) # from (2, 4) to (4, 2)\n",
    "\n",
    "print(f'P0 shape: {P0.shape}\\nP1 shape: {P1.shape}\\nP shape: {np.array(P).shape}\\nR shape: {R.shape}\\nR_all_actions shape: {R_all_actions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8b3726e8-452c-484e-aabe-51b9371883dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: (0, 1, 0, 0)\n",
      "Optimal value function: (12.5927089168427, 4.372077720880815, -0.5770590796636772, 0.0)\n"
     ]
    }
   ],
   "source": [
    "mdp = mdptoolbox.mdp.ValueIteration(P, R_all_actions, gamma)\n",
    "mdp.run()\n",
    "\n",
    "print(\"Optimal policy:\", mdp.policy)  # 最优策略（对每个状态选择哪一个动作）\n",
    "print(\"Optimal value function:\", mdp.V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0237b3-f15e-4b10-980d-0868dfee7957",
   "metadata": {},
   "source": [
    "![image](https://github.com/jiaowoguanren0615/FIT-5226/blob/main/images/week4_lab.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8323b120-4c9f-4d86-8d3f-c6dfdc794fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_single = np.array([\n",
    "    # home   shelter  dead   city   market\n",
    "    [0.5,    0.0,     0.1,    0.4,   0.0   ],  # from home\n",
    "    [0.0,    0.2,     0.2,    0.2,   0.4   ],  # from shelter\n",
    "    [0.0,    0.0,     1.0,    0.0,   0.0   ],  # from dead (吸收)\n",
    "    [0.1,    0.0,     0.4,    0.5,   0.0   ],  # from city\n",
    "    [0.0,    0.0,     0.4,    0.0,   0.6   ],  # from market\n",
    "])\n",
    "\n",
    "R_single = np.array([0.4, 0.2, 0.0, 0.3, 0.7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c79c141b-0e4e-49ce-bd07-c4c19a086f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5) (5,)\n"
     ]
    }
   ],
   "source": [
    "print(P_single.shape, R_single.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2617dd21-967e-4950-96b2-b8ee519fd894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_single = np.expand_dims(P_single, axis=0)\n",
    "P_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0df9d0ca-55a5-4f9c-9fdb-2706a042da54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_single = np.expand_dims(R_single, axis=1)\n",
    "R_single.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db715601-b7c2-4802-bc91-5e4a6431f2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy: (0, 0, 0, 0, 0)\n",
      "Optimal value function: (1.213044130367309, 1.0749256662104465, 0.0, 0.7435083661488171, 1.5215918155583958)\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9  # 折扣因子\n",
    "\n",
    "# 用 ValueIteration\n",
    "mdp = mdptoolbox.mdp.ValueIteration(P_single, R_single, gamma)\n",
    "mdp.run()\n",
    "\n",
    "print(\"Optimal policy:\", mdp.policy)\n",
    "print(\"Optimal value function:\", mdp.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76411501-252b-40a4-9b5c-1842ee722e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "torch2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
